{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment_2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1yOLQBXFScOVJbCUlBFRG5sdL_ODpXt6u","authorship_tag":"ABX9TyNH/uHpAeyVUq8dU/OmGEDr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# install the package\n","!pip install OpenNMT-py"],"metadata":{"id":"-8j4MQ0giWY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import codecs\n","import pickle as cPickle\n","import random\n","from collections import defaultdict\n","import pandas as pd\n","import re"],"metadata":{"id":"0Lu_rN_zUkVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createDatasets(corpus_file,sequence=\"grapheme\"):\n","  \"\"\"\n","  generate train, valid, test sets. (Match train/test split from experiment 1, by lemma.)\n","  \"\"\"\n","\n","  #set up output file\n","  fout_src_train = codecs.open(r'/experiment_2/src_train_tagged.txt','wb','utf-8')\n","  fout_tgt_train = codecs.open(r'/experiment_2/tgt_train_tagged.txt','wb','utf-8')\n","  fout_src_valid = codecs.open(r'/experiment_2/src_valid_tagged.txt','wb','utf-8')\n","  fout_src_test = codecs.open(r'/experiment_2/src_test_tagged.txt','wb','utf-8')\n","\n","\n","  #modify every line in the current valid data\n","  fin = codecs.open(r'/experiment_1/src_valid.txt','rb','utf-8')\n","  for line in fin:\n","    fout_src_valid.write('<V;PST> ' + line)\n","  fin.close()\n","  #modify every line in the current test data\n","  fin = codecs.open(r'/experiment_1/src_test.txt','rb','utf-8')\n","  for line in fin:\n","    fout_src_test.write('<V;PST> ' + line)\n","  fin.close()\n","\n","  #read in a set of valid lemmas from the current train data\n","  ok_lemmas = set()\n","  fin =  codecs.open(r'/experiment_1/src_train.txt','rb','utf-8')\n","  for line in fin:\n","    ok_lemmas.add(line.strip())\n","  fin.close()\n","\n","  #read in data\n","  fin = codecs.open(r'/experiment_2/english_merged.txt','rb','utf-8')\n","\n","  sources = []\n","  targets = []\n","\n","  if sequence == \"grapheme\"\n","    for line in fin:\n","      parts = line.strip().split()\n","      lemma = parts[3]\n","      form = parts[4]\n","      vec = '<' + parts[2] + '> '\n","      if vec != 'V;NFIN' and ' '.join(lemma) in ok_lemmas:\n","        sources.append(vec + ' '.join(lemma))\n","        targets.append(' '.join(form))\n","  else: \n","    for line in fin:\n","      parts = line.strip().split()\n","      lemma = parts[0]\n","      form = parts[1]\n","      vec = '<' + parts[2] + '> '\n","      if vec != 'V;NFIN' and ' '.join(lemma) in ok_lemmas:\n","        sources.append(vec + ' '.join(lemma))\n","        targets.append(' '.join(form))\n","  fin.close()\n","\n","  pairs = list(zip(sources,targets))\n","  random.seed(222)\n","  random.shuffle(pairs)\n","\n","  #split into train and test\n","  train = pairs\n","\n","  #write the outputs\n","  for s,t in train:\n","    fout_src_train.write(s + '\\n')\n","    fout_tgt_train.write(t + '\\n')\n","\n","  fout_src_train.close()\n","  fout_tgt_train.close()\n","  fout_src_valid.close()\n","  fout_src_test.close()"],"metadata":{"id":"TrJ6TmSIpLw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def DataFrame_merged(merged_file):\n","  \"\"\"\n","  read the merged file and convert it to a dataframe, and get some basic information about it.\n","  \"\"\"\n","\n","  data = pd.read_csv(merged_file,sep='\\t',names=[\"pre_tense\",\"past_tense\",\"IPA_pre\",\"IPA_past\",\"label\"])\n","  print(data[:5])\n","\n","  print(\"number of regular verbs:\", len(data.loc[data[\"label\"]==\"reg\"]))\n","  print(\"number of irregular verbs:\", len(data.loc[data[\"label\"]==\"irreg\"]))\n","  return data\n","  \n","def DataFrame_file(filename,src=True):\n","  '''\n","  read the file and convert it to a dataframe\n","  '''\n","  with open(filename,'r') as f:\n","    if src:\n","      task, pre = [], []\n","      for line in f:\n","        task.append(re.findall(\"<.+>\",line.strip('\\n'))[0].replace(\"<\",\"\").replace(\">\",\"\"))\n","        pre.append(re.sub(\"<.+>\",\"\",line.strip('\\n')).replace(\" \",\"\"))\n","        df = pd.DataFrame({\"task\":task, \"IPA_pre\":pre})\n","    else:\n","      list_file = [line.strip('\\n').replace(\" \",\"\") for line in f]\n","      df = pd.DataFrame(list_file)\n","    return df"],"metadata":{"id":"CkcfyFtKUag-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def results_set(file_pre,file_src,file_tgt):\n","  '''\n","  calculate the accuracy of regular and irregular verbs in the set and the accuracy of the set, \n","  and return them\n","  '''\n","  df_pre = DataFrame_file(file_pre,src=False)\n","  df_src = DataFrame_file(file_src,src=True)\n","  df_tgt = DataFrame_file(file_tgt,src=False)\n","  df_set = pd.concat([df_pre,df_src,df_tgt],axis=1)\n","\n","  if sequence == \"grapheme\":\n","    df_set.columns = [\"prediction\",\"task\",\"pre_tense\",\"past_tense\"]\n","    df_set_merged = pd.merge(data, df_set)\n","    df_set_wrong = df_set_merged.loc[df_set_merged[\"prediction\"]!=df_set_merged[\"past_tense\"]]\n","  else:\n","    df_set.columns = [\"prediction\",\"task\",\"IPA_pre\",\"IPA_past\"]\n","    df_set_merged = pd.merge(data, df_set)\n","    df_set_wrong = df_set_merged.loc[df_set_merged[\"prediction\"]!=df_set_merged[\"IPA_past\"]]\n","\n","  total_reg = len(df_set_merged.loc[df_set_merged[\"label\"]==\"reg\"])\n","  total_ir = len(df_set_merged.loc[df_set_merged[\"label\"]==\"irreg\"])\n","  wrong_reg = len(df_set_wrong.loc[df_set_wrong[\"label\"]==\"reg\"])\n","  wrong_ir = len(df_set_wrong.loc[df_set_wrong[\"label\"]==\"irreg\"])\n","\n","  print(\"Set\\tRegular\\tIrregular\\n\"+\"-\"*40)\n","  print(f\"{round(1-len(df_set_wrong)/len(df_set),3)}\\t\"\n","    f\"{round(1-wrong_reg/total_reg,3)}\\t\"\n","    f\"{round(1-wrong_ir/total_ir,3)}\"\n","    )"],"metadata":{"id":"KvOfVaZsUdHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!onmt_build_vocab -config /experiment_2/config.yaml -n_sample 75200\n","!onmt_train -config /experiment_2/config.yaml"],"metadata":{"id":"R5QbpMPilihh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!onmt_translate -model /experiment_2/run/model_step_75200.pt -src /experiment_2/src_train_tagged.txt -output experiment_2/pre_train.txt\n","!onmt_translate -model /experiment_2/run/model_step_75200.pt -src /experiment_2/src_valid_tagged.txt -output experiment_2/pre_valid.txt\n","!onmt_translate -model /experiment_2/run/model_step_75200.pt -src /experiment_2/src_test_tagged.txt -output experiment_2/pre_test.txt"],"metadata":{"id":"iCzYuj0PmIPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_file = \"experiment_2/english_merged.txt\"\n","data = DataFrame_merged(merged_file)\n","\n","tgt_train = \"experiment_2/tgt_train_tagged.txt\"\n","pre_train = \"experiment_2/pre_train.txt\"\n","src_train = \"experiment_2/src_train_tagged.txt\"\n","print(\"train set:\")\n","evaluation(tgt_train,pre_train,src_train)\n","\n","tgt_valid = \"experiment_1/tgt_valid.txt\"\n","pre_valid = \"experiment_2/pre_valid.txt\"\n","src_valid = \"experiment_2/src_valid_tagged.txt\"\n","print(\"valid set:\")\n","evaluation(tgt_valid,pre_valid,src_valid)\n","\n","tgt_test = \"experiment_1/tgt_test.txt\"\n","pre_test = \"experiment_2/pre_test.txt\"\n","src_test = \"experiment_2/src_test_tagged.txt\"\n","print(\"test set:\")\n","evaluation(tgt_test,pre_test,src_test)"],"metadata":{"id":"YFcwSuIHjM9N"},"execution_count":null,"outputs":[]}]}