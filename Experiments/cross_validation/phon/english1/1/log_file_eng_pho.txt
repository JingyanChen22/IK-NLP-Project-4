[2022-10-10 12:13:44,927 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2022-10-10 12:13:44,928 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2022-10-10 12:13:44,928 INFO] Missing transforms field for valid data, set to default: [].
[2022-10-10 12:13:44,928 INFO] Parsed 2 corpora from -data.
[2022-10-10 12:13:44,928 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.
[2022-10-10 12:13:44,928 INFO] Loading vocab from text file...
[2022-10-10 12:13:44,928 INFO] Loading src vocabulary from cross_validation/phon/english1/1/example.vocab.src
[2022-10-10 12:13:44,929 INFO] Loaded src vocab has 38 tokens.
[2022-10-10 12:13:44,929 INFO] Loading tgt vocabulary from cross_validation/phon/english1/1/example.vocab.tgt
[2022-10-10 12:13:44,929 INFO] Loaded tgt vocab has 39 tokens.
[2022-10-10 12:13:44,929 INFO] Building fields with vocab in counters...
[2022-10-10 12:13:44,930 INFO]  * tgt vocab size: 43.
[2022-10-10 12:13:44,930 INFO]  * src vocab size: 40.
[2022-10-10 12:13:44,930 INFO]  * src vocab size = 40
[2022-10-10 12:13:44,930 INFO]  * tgt vocab size = 43
[2022-10-10 12:13:44,931 INFO] Building model...
[2022-10-10 12:13:44,974 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(40, 500, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(500, 50, num_layers=2, dropout=0.3, bidirectional=True)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(43, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(600, 100)
        (1): LSTMCell(100, 100)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=100, out_features=100, bias=False)
      (linear_out): Linear(in_features=200, out_features=100, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=100, out_features=43, bias=True)
    (1): Cast()
    (2): LogSoftmax(dim=-1)
  )
)
[2022-10-10 12:13:44,975 INFO] encoder: 301600
[2022-10-10 12:13:44,975 INFO] decoder: 417443
[2022-10-10 12:13:44,975 INFO] * number of parameters: 719043
[2022-10-10 12:13:50,755 INFO] Starting training on CPU, could be very slow
[2022-10-10 12:13:50,755 INFO] Start training loop and validate every 10000 steps...
[2022-10-10 12:13:50,756 INFO] corpus_1's transforms: TransformPipe()
[2022-10-10 12:13:50,756 INFO] Weighted corpora loaded so far:
			* corpus_1: 1
[2022-10-10 12:13:58,676 INFO] Weighted corpora loaded so far:
			* corpus_1: 2
[2022-10-10 12:14:04,391 INFO] Step 244/16155; acc:  25.01; ppl: 14.94; xent: 2.70; lr: 1.00000; 2166/2893 tok/s;     14 sec
[2022-10-10 12:14:07,014 INFO] Weighted corpora loaded so far:
			* corpus_1: 3
[2022-10-10 12:14:10,904 INFO] Weighted corpora loaded so far:
			* corpus_1: 4
[2022-10-10 12:14:13,804 INFO] Step 488/16155; acc:  77.09; ppl:  2.42; xent: 0.88; lr: 1.00000; 3180/4230 tok/s;     23 sec
[2022-10-10 12:14:18,744 INFO] Weighted corpora loaded so far:
			* corpus_1: 5
[2022-10-10 12:14:22,665 INFO] Weighted corpora loaded so far:
			* corpus_1: 6
[2022-10-10 12:14:23,045 INFO] Step 732/16155; acc:  96.76; ppl:  1.21; xent: 0.19; lr: 1.00000; 3221/4265 tok/s;     32 sec
[2022-10-10 12:14:30,464 INFO] Weighted corpora loaded so far:
			* corpus_1: 7
[2022-10-10 12:14:32,374 INFO] Step 976/16155; acc:  98.40; ppl:  1.10; xent: 0.09; lr: 1.00000; 3162/4239 tok/s;     42 sec
[2022-10-10 12:14:38,641 INFO] Weighted corpora loaded so far:
			* corpus_1: 8
[2022-10-10 12:14:42,170 INFO] Step 1220/16155; acc:  98.63; ppl:  1.08; xent: 0.07; lr: 1.00000; 3080/4081 tok/s;     51 sec
[2022-10-10 12:14:43,559 INFO] Weighted corpora loaded so far:
			* corpus_1: 9
[2022-10-10 12:14:55,722 INFO] Weighted corpora loaded so far:
			* corpus_1: 10
[2022-10-10 12:14:56,534 INFO] Step 1464/16155; acc:  98.67; ppl:  1.07; xent: 0.07; lr: 1.00000; 2067/2751 tok/s;     66 sec
[2022-10-10 12:14:59,623 INFO] Weighted corpora loaded so far:
			* corpus_1: 11
[2022-10-10 12:15:02,747 INFO] Saving checkpoint cross_validation/phon/english1/1/model_step_1615.pt
[2022-10-10 12:15:07,450 INFO] Step 1708/16155; acc:  99.11; ppl:  1.05; xent: 0.05; lr: 1.00000; 2691/3610 tok/s;     77 sec
[2022-10-10 12:15:10,956 INFO] Weighted corpora loaded so far:
			* corpus_1: 12
[2022-10-10 12:15:15,087 INFO] Weighted corpora loaded so far:
			* corpus_1: 13
[2022-10-10 12:15:19,045 INFO] Step 1952/16155; acc:  98.99; ppl:  1.05; xent: 0.05; lr: 1.00000; 2616/3454 tok/s;     88 sec
[2022-10-10 12:15:23,581 INFO] Weighted corpora loaded so far:
			* corpus_1: 14
[2022-10-10 12:15:29,350 INFO] Step 2196/16155; acc:  99.01; ppl:  1.05; xent: 0.04; lr: 1.00000; 2875/3833 tok/s;     99 sec
[2022-10-10 12:15:32,168 INFO] Weighted corpora loaded so far:
			* corpus_1: 15
[2022-10-10 12:15:36,332 INFO] Weighted corpora loaded so far:
			* corpus_1: 16
[2022-10-10 12:15:39,445 INFO] Step 2440/16155; acc:  99.25; ppl:  1.04; xent: 0.03; lr: 1.00000; 2947/3932 tok/s;    109 sec
[2022-10-10 12:15:45,161 INFO] Weighted corpora loaded so far:
			* corpus_1: 17
[2022-10-10 12:15:49,159 INFO] Weighted corpora loaded so far:
			* corpus_1: 18
[2022-10-10 12:15:49,374 INFO] Step 2684/16155; acc:  99.23; ppl:  1.04; xent: 0.04; lr: 1.00000; 3011/3980 tok/s;    119 sec
